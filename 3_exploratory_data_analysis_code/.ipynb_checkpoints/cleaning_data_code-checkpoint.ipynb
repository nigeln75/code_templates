{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by reading in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding files that match a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the glob module to find all csv files in the workspace. \n",
    "# The glob module has a function called glob that takes a pattern and returns a list of the files in the working directory that match that pattern\n",
    "# For example, if you know the pattern is part_ single digit number .csv, you can write the pattern as 'part_?.csv' (which would match part_1.csv, part_2.csv, part_3.csv, etc.)\n",
    "# Similarly, you can find all .csv files with '*.csv', or all parts with 'part_*'\n",
    "# The ? wildcard represents any 1 character, and the * wildcard represents any number of characters\n",
    "\n",
    "# Import necessary modules\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = '*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Load the second file into a DataFrame: csv2\n",
    "csv2 = pd.read_csv(csv_files[1])\n",
    "\n",
    "# Print the head of csv2\n",
    "print(csv2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating and concatenating all matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)\n",
    "\n",
    "# Print the shape of uber\n",
    "print(uber.shape)\n",
    "\n",
    "# Print the head of uber\n",
    "print(uber.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting data types to categorical data to reduce memory usage (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the sex column to type 'category'\n",
    "tips.sex = tips.sex.astype('category')\n",
    "\n",
    "# Convert the smoker column to type 'category'\n",
    "tips.smoker = tips.smoker.astype('category')\n",
    "\n",
    "# Print the info of tips\n",
    "print(tips.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can use the pd.to_numeric() function to convert a column into a numeric data type\n",
    "# If the function raises an error, you can be sure that there is a bad value within the column\n",
    "# In which case either do some EDA to find the bad value, or ignore or coerce the value into a missing value, NaN\n",
    "\n",
    "# Convert 'total_bill' to a numeric dtype\n",
    "tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')\n",
    "\n",
    "# Convert 'tip' to a numeric dtype\n",
    "tips['tip'] = pd.to_numeric(tips['tip'], errors='coerce')\n",
    "\n",
    "# Print the info of tips\n",
    "print(tips.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String parsing with regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# When working with data, it is sometimes necessary to write a regular expression to look for properly entered values\n",
    "# Phone numbers in a dataset is a common field that needs to be checked for validity\n",
    "# Define a regular expression to match US phone numbers that fit the pattern of xxx-xxx-xxxx\n",
    "\n",
    "# The regular expression module in python is re\n",
    "# Since the pattern will be used for a match across multiple rows, it's better to compile the pattern first using re.compile()\n",
    "# You can then use the compiled pattern to match values\n",
    "\n",
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Compile the pattern: prog\n",
    "prog = re.compile('\\d{3}-\\d{3}-\\d{4}')\n",
    "\n",
    "# See if the pattern matches\n",
    "result = prog.match('123-456-7890')\n",
    "print(bool(result))\n",
    "\n",
    "# See if the pattern matches\n",
    "result = prog.match('1123-456-7890')\n",
    "print(bool(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting numerical values from strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '1']\n"
     ]
    }
   ],
   "source": [
    "# Extracting numbers from strings is a common task, particularly when working with unstructured data or log files\n",
    "# Say you have the following string: 'the recipe calls for 6 strawberries and 2 bananas'\n",
    "\n",
    "# It would be useful to extract the 6 and the 2 to be saved for later use when comparing strawberry to banana ratios\n",
    "# When using a regular expression to extract multiple numbers (or multiple pattern matches, to be exact), you can use the re.findall() function\n",
    "# You pass in a pattern and a string to re.findall(), and it will return a list of the matches\n",
    "\n",
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Find the numeric values: matches\n",
    "matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana')\n",
    "\n",
    "# Print the matches\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Use \\$ to match the dollar sign, \\d* to match an arbitrary number of digits, \\. to match the decimal point, and \\d{x} to match x number of digit\n",
    "\n",
    "# Write the first pattern\n",
    "print(bool(re.match(pattern='\\d{3}-\\d{3}-\\d{4}', string='123-456-7890')))\n",
    "\n",
    "# Write the second pattern\n",
    "print(bool(re.match(pattern='\\$\\d*\\.\\d{2}', string='$123.45')))\n",
    "\n",
    "# Write the third pattern\n",
    "print(bool(re.match(pattern='[A-Z]\\w*', string='Australia')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for Gapminder data - looking at country spellings (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See if there are any special or invalid characters you may need to deal with\n",
    "# It is reasonable to assume that country names will contain:\n",
    "# - The set of lower and upper case letters\n",
    "# - Whitespace between words\n",
    "# - Periods for any abbreviations\n",
    "\n",
    "# To confirm that this is the case, you can leverage the power of regular expressions again\n",
    "# For common operations like this, Python has a built-in string method - str.contains()\n",
    "# This takes a regular expression pattern, and applies it to the Series, returning True if there is a match, and False otherwise\n",
    "\n",
    "# Since here you want to find the values that do not match, you have to invert the boolean, which can be done using ~ \n",
    "# This Boolean series can then be used to get the Series of countries that have invalid names\n",
    "\n",
    "# Create the series of countries: countries\n",
    "countries = gapminder['country']\n",
    "\n",
    "# Drop all the duplicates from countries\n",
    "countries = countries.drop_duplicates()\n",
    "\n",
    "# Write the regular expression: pattern\n",
    "# Anchor the pattern to match exactly what you want by placing a ^ in the beginning and $ in the end\n",
    "pattern = '^[A-Za-z\\.\\s]*$'\n",
    "\n",
    "# Create the Boolean vector: mask\n",
    "# Use str.contains() to create a Boolean vector representing values that match the pattern\n",
    "mask = countries.str.contains(pattern)\n",
    "\n",
    "# Invert the mask: mask_inverse\n",
    "# Invert the mask by placing a ~ before it\n",
    "mask_inverse = ~mask\n",
    "\n",
    "# Subset countries using mask_inverse: invalid_countries\n",
    "invalid_countries = countries.loc[mask_inverse]\n",
    "\n",
    "# Print invalid_countries\n",
    "print(invalid_countries)\n",
    "\n",
    "93               Guinea-Bissau\n",
    "    98            Hong Kong, China\n",
    "    118    United Korea (former)\\n\n",
    "    131               Macao, China\n",
    "    132             Macedonia, FYR\n",
    "    145      Micronesia, Fed. Sts.\n",
    "    161            Ngorno-Karabakh\n",
    "    187             St. Barth?lemy\n",
    "    193     St.-Pierre-et-Miquelon\n",
    "    225                Timor-Leste\n",
    "    251      Virgin Islands (U.S.)\n",
    "    252       North Yemen (former)\n",
    "    253       South Yemen (former)\n",
    "    258                      ?land"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing functions to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your job is to write a function that will recode 'Male' to 1, 'Female' to 0, and return np.nan for all entries of 'sex' that are neither 'Male' nor 'Female'.\n",
    "# Recoding variables like this is a common data cleaning task\n",
    "# Functions provide a mechanism for you to abstract away complex bits of code as well as reuse code\n",
    "# This makes your code more readable and less error prone\n",
    "\n",
    "# You can use the .apply() method to apply a function across entire rows or columns of DataFrames\n",
    "# However, note that each column of a DataFrame is a pandas Series\n",
    "# Functions can also be applied across Series. Here, you will apply your function over the 'sex' column\n",
    "\n",
    "# Define recode_sex()\n",
    "def recode_sex(sex_value):\n",
    "\n",
    "    # Return 1 if sex_value is 'Male'\n",
    "    if sex_value == 'Male':\n",
    "        return 1\n",
    "    \n",
    "    # Return 0 if sex_value is 'Female'\n",
    "    elif sex_value == 'Female':\n",
    "        return 0\n",
    "        \n",
    "    # Return np.nan    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the sex column\n",
    "tips['sex_recode'] = tips.sex.apply(recode_sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lambda functions can make your code concise and Pythonic\n",
    "# our job is to clean its 'total_dollar' column by removing the dollar sign\n",
    "# You'll do this using two different methods: With the .replace() method, and with regular expressions\n",
    "# The regular expression module re has been pre-imported.\n",
    "\n",
    "# Write the lambda function using replace\n",
    "tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))\n",
    "\n",
    "# Write the lambda function using regular expressions\n",
    "tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\\d+\\.\\d+', x))\n",
    "\n",
    "# Print the head of tips\n",
    "print(tips.head())\n",
    "\n",
    "<script.py> output:\n",
    "       total_bill   tip     sex smoker  day    time  size total_dollar  \\\n",
    "    0       16.99  1.01  Female     No  Sun  Dinner     2       $16.99   \n",
    "    1       10.34  1.66    Male     No  Sun  Dinner     3       $10.34   \n",
    "    2       21.01  3.50    Male     No  Sun  Dinner     3       $21.01   \n",
    "    3       23.68  3.31    Male     No  Sun  Dinner     2       $23.68   \n",
    "    4       24.59  3.61  Female     No  Sun  Dinner     4       $24.59   \n",
    "    \n",
    "      total_dollar_replace total_dollar_re  \n",
    "    0                16.99         [16.99]  \n",
    "    1                10.34         [10.34]  \n",
    "    2                21.01         [21.01]  \n",
    "    3                23.68         [23.68]  \n",
    "    4                24.59         [24.59]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning starts with basic exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read the file into a DataFrame: df\n",
    "df = pd.read_csv('dob_job_application_filings_subset.csv')\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Print the tail of df\n",
    "print(df.tail())\n",
    "\n",
    "# Print the shape of df\n",
    "print(df.shape)\n",
    "\n",
    "# Print the columns of df\n",
    "print(df.columns)\n",
    "\n",
    "# Print the head and tail of df_subset\n",
    "print(df_subset.head())\n",
    "print(df_subset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the info of df\n",
    "print(df.info())\n",
    "\n",
    "# Print the info of df_subset\n",
    "print(df_subset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Duplicate data causes a variety of problems\n",
    "# From the point of view of performance, they use up unnecessary amounts of memory and cause unneeded calculations to be performed when processing data\n",
    "# In addition, they can also bias any analysis results\n",
    "\n",
    "# Create the new DataFrame: tracks\n",
    "tracks = billboard[['year', 'artist', 'track', 'time']]\n",
    "\n",
    "# Print info of tracks\n",
    "print(tracks.info())\n",
    "\n",
    "# Drop the duplicates: tracks_no_duplicates\n",
    "tracks_no_duplicates = tracks.drop_duplicates()\n",
    "\n",
    "# Print info of tracks\n",
    "print(tracks_no_duplicates.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It's rare to have a (real-world) dataset without any missing values\n",
    "# Certain calculations cannot handle missing values while some calculations will, by default, skip over missing values\n",
    "# Understand how much missing data you have, and where missing data comes from\n",
    "# This will ensure you make unbiased interpretations of data\n",
    "\n",
    "# Calculate the mean of the Ozone column: oz_mean\n",
    "oz_mean = airquality.Ozone.mean()\n",
    "\n",
    "# Replace all the missing values in the Ozone column with the mean\n",
    "airquality['Ozone'] = airquality.Ozone.fillna(oz_mean)\n",
    "\n",
    "# Print the info of airquality\n",
    "print(airquality.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping missing data - Gapminder example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In general, it is not good to drop missing values, because you may end up throwing away useful information\n",
    "# In this data, missing values refer to years where no estimate for life expectancy is available for a given country\n",
    "# You could fill in, or guess what these life expectancies could be by looking at the average life expectancies for other countries in that year, for example\n",
    "# Whichever strategy you go with, it is important to carefully consider all options and understand how they will affect your data\n",
    "\n",
    "# Your job is to drop all the rows that have NaN in the life_expectancy column\n",
    "# Before doing so, it would be valuable to use assert statements to confirm that year and country do not have any missing values\n",
    "\n",
    "# If there is no reasonable way to fill in or impute missing values, then dropping the missing data may be the best solution\n",
    "\n",
    "# Assert that country does not contain any missing values\n",
    "assert pd.notnull(gapminder.country).all()\n",
    "\n",
    "# Assert that year does not contain any missing values\n",
    "assert pd.notnull(gapminder.year).all()\n",
    "\n",
    "# Drop the missing values\n",
    "# Drop the values in the data where life_expectancy is missing\n",
    "# As you confirmed that country and year don't have missing values\n",
    "# You can use the .dropna() method on the entire gapminder DataFrame, because any missing values would have to be in the life_expectancy column\n",
    "# Specify the keyword argument how='any\n",
    "gapminder = gapminder.dropna(how='any')\n",
    "\n",
    "# Print the shape of gapminder\n",
    "print(gapminder.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your data with asserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the .all() method together with the .notnull() DataFrame method to check for missing values in a column\n",
    "# The .all() method returns True if all values are True\n",
    "# When used on a DataFrame, it returns a Series of Booleans - one for each column in the DataFrame\n",
    "# So if you are using it on a DataFrame, like in this exercise, you need to chain another .all() method so that you return only one True or False value\n",
    "\n",
    "# Note: You can use pd.notnull(df) as an alternative to df.notnull()\n",
    "\n",
    "# Assert that there are no missing values\n",
    "# Use the pd.notnull() function on ebola (or the .notnull() method of ebola) and chain two .all() methods\n",
    "# That is, .all().all())\n",
    "# The first .all() method will return a True or False for each column\n",
    "# The second .all() method will return a single True or False\n",
    "assert ebola.notnull().all().all()\n",
    "\n",
    "# Assert that all values are >= 0\n",
    "assert (ebola >= 0).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another example!\n",
    "# Convert the year column to numeric\n",
    "gapminder.year = pd.to_numeric(gapminder.year)\n",
    "\n",
    "# Test if country is of type object\n",
    "assert gapminder.country.dtypes == np.object\n",
    "\n",
    "# Test if year is of type int64\n",
    "assert gapminder.year.dtypes == np.int64\n",
    "\n",
    "# Test if life_expectancy is of type float64\n",
    "assert gapminder.life_expectancy.dtypes == np.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get in the habit of testing your data with asserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function to check the following:\n",
    "# - 'Life expectancy' is the first column (index 0) of the DataFrame\n",
    "# - The other columns contain either null or numeric values\n",
    "# - The numeric values are all greater than or equal to 0\n",
    "# - There is only one instance of each country\n",
    "\n",
    "def check_null_or_valid(row_data):\n",
    "    \"\"\"Function that takes a row a data,\n",
    "    drops all missing values,\n",
    "    and checks if all remaining values are greater than or equal to 0\n",
    "    \"\"\"\n",
    "    no_na = row_data.dropna()[1:-1]\n",
    "    numeric = pd.to_numeric(no_na)\n",
    "    ge0 = numeric >= 0\n",
    "    return ge0\n",
    "\n",
    "# Check whether the first column is 'Life expectancy'\n",
    "assert g1800s.columns[0] == 'Life expectancy'\n",
    "\n",
    "# Check whether the values in the row are valid\n",
    "assert g1800s.iloc[:, 1:].apply(check_null_or_valid, axis=1).all().all()\n",
    "\n",
    "# Check that there is only one instance of each country\n",
    "assert g1800s['Life expectancy'].value_counts()[0] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency counts for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the value counts for 'Borough'\n",
    "print(df['Borough'].value_counts(dropna=False))\n",
    "\n",
    "# Print the value_counts for 'State'\n",
    "print(df['State'].value_counts(dropna=False))\n",
    "\n",
    "# Print the value counts for 'Site Fill'\n",
    "print(df['Site Fill'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing single variables with histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You'll notice that there are extremely large differences between the min and max values, and the plot will need to be adjusted accordingly\n",
    "# In such cases, it's good to look at the plot on a log scale\n",
    "# The keyword arguments logx=True or logy=True can be passed in to .plot() depending on which axis you want to rescale\n",
    "\n",
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the histogram\n",
    "df['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=True, logy=True)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing multiple variables with boxplots (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the boxplot\n",
    "df.boxplot(column='initial_cost', by='Borough', rot=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing multiple variables with scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create and display the first scatter plot\n",
    "df.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)\n",
    "plt.show()\n",
    "\n",
    "# Create and display the second scatter plot\n",
    "df_subset.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the scatter plot\n",
    "g1800s.plot(kind='scatter', x='1800', y='1899')\n",
    "\n",
    "# Specify axis labels\n",
    "plt.xlabel('Life Expectancy by Country in 1800')\n",
    "plt.ylabel('Life Expectancy by Country in 1899')\n",
    "\n",
    "# Specify axis limits\n",
    "plt.xlim(20, 55)\n",
    "plt.ylim(20, 55)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add first subplot\n",
    "plt.subplot(2, 1, 1) \n",
    "\n",
    "# Create a histogram of life_expectancy\n",
    "gapminder.life_expectancy.plot(kind='hist')\n",
    "\n",
    "# Group gapminder: gapminder_agg\n",
    "gapminder_agg = gapminder.groupby('year')['life_expectancy'].mean()\n",
    "\n",
    "# Print the head of gapminder_agg\n",
    "print(gapminder_agg.head())\n",
    "\n",
    "# Print the tail of gapminder_agg\n",
    "print(gapminder_agg.tail())\n",
    "\n",
    "# Add second subplot\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Create a line plot of life expectancy per year\n",
    "gapminder_agg.plot()\n",
    "\n",
    "# Add title and specify axis labels\n",
    "plt.title('Life expectancy over the years')\n",
    "plt.ylabel('Life expectancy')\n",
    "plt.xlabel('Year')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save both DataFrames to csv files\n",
    "gapminder.to_csv('gapminder.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping your data using melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Melting data is the process of turning columns of your data into rows of data\n",
    "Consider the DataFrames from the previous exercise. \n",
    "In the tidy DataFrame, the variables Ozone, Solar.R, Wind, and Temp each had their own column.\n",
    "If, however, you wanted these variables to be in rows instead, you could melt the DataFrame. \n",
    "In doing so, however, you would make the data untidy! \n",
    "This is important to keep in mind: Depending on how your data is represented, you will have to reshape it differently.\n",
    "\n",
    "In this exercise, you will practice melting a DataFrame using pd.melt(). \n",
    "There are two parameters you should be aware of: id_vars and value_vars. \n",
    "The id_vars represent the columns of the data you do not want to melt (i.e., keep it in its current shape), while the value_vars represent the columns you do wish to melt into rows. \n",
    "By default, if no value_vars are provided, all columns not set in the id_vars will be melted. \n",
    "This could save a bit of typing, depending on the number of columns that need to be melted.\n",
    "\n",
    "The (tidy) DataFrame airquality has been pre-loaded. \n",
    "Your job is to melt its Ozone, Solar.R, Wind, and Temp columns into rows. \n",
    "Later in this chapter, you'll learn how to bring this melted DataFrame back into a tidy form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the head of airquality\n",
    "print(airquality.head())\n",
    "\n",
    "# Melt airquality: airquality_melt\n",
    "airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day'])\n",
    "\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing melted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the head of airquality\n",
    "print(airquality.head())\n",
    "\n",
    "# Melt airquality: airquality_melt\n",
    "airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day'], var_name='measurement', value_name='reading')\n",
    "\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping your data - Gapminder data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Currently, the gapminder DataFrame has a separate column for each year\n",
    "# What you want instead is a single column that contains the year, and a single column that represents the average life expectancy for each year and country\n",
    "# By having year in its own column, you can use it as a predictor variable in a later analysis\n",
    "\n",
    "# You can convert the DataFrame into the desired tidy format by melting it\n",
    "\n",
    "# Melt gapminder: gapminder_melt\n",
    "gapminder_melt = pd.melt(gapminder, id_vars='Life expectancy')\n",
    "\n",
    "# Rename the columns\n",
    "gapminder_melt.columns = ['country','year','life_expectancy']\n",
    "\n",
    "# Print the head of gapminder_melt\n",
    "print(gapminder_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# .pivot_table() has an index parameter which you can use to specify the columns that you don't want pivoted\n",
    "# It is similar to the id_vars parameter of pd.melt()\n",
    "# Two other parameters that you have to specify are columns (the name of the column you want to pivot), and values (the values to be used when the column is pivoted)\n",
    "\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())\n",
    "\n",
    "# Pivot airquality_melt: airquality_pivot\n",
    "airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading')\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resetting the index of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After pivoting airquality_melt in the previous exercise, you didn't quite get back the original DataFrame\n",
    "# What you got back instead was a pandas DataFrame with a hierarchical index (also known as a MultiIndex)\n",
    "# In essence, they allow you to group columns or rows by another variable - in this case, by 'Month' as well as 'Day'\n",
    "# To get back the original DataFrame from the pivoted DataFrame use .reset_index()\n",
    "\n",
    "# Print the index of airquality_pivot\n",
    "print(airquality_pivot.index)\n",
    "\n",
    "# Reset the index of airquality_pivot: airquality_pivot\n",
    "airquality_pivot = airquality_pivot.reset_index()\n",
    "\n",
    "# Print the new index of airquality_pivot\n",
    "print(airquality_pivot.index)\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also use pivot tables to deal with duplicate values by providing an aggregation function through the aggfunc parameter\n",
    "# Let's say your data collection method accidentally duplicated your dataset. \n",
    "# Such a dataset, in which each row is duplicated, has been pre-loaded as airquality_dup\n",
    "# In addition, the airquality_melt DataFrame from the previous exercise has been pre-loaded\n",
    "\n",
    "# By using .pivot_table() and the aggfunc parameter, you can not only reshape your data, but also remove duplicates!\n",
    "# Finally, you can then flatten the columns of the pivoted DataFrame using .reset_index()\n",
    "\n",
    "# Pivot airquality_dup: airquality_pivot\n",
    "airquality_pivot = airquality_dup.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading', aggfunc=np.mean)\n",
    "\n",
    "# Reset the index of airquality_pivot\n",
    "airquality_pivot = airquality_pivot.reset_index()\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())\n",
    "\n",
    "# Print the head of airquality\n",
    "print(airquality.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting a column with .str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ou're going to tidy the 'm014' column, which represents males aged 0-14 years of age\n",
    "# In order to parse this value, you need to extract the first letter into a new column for gender, and the rest into a column for age_group\n",
    "# Here, since you can parse values by position, you can take advantage of pandas' vectorized string slicing by using the str attribute of columns of type object\n",
    "\n",
    "# Melt tb: tb_melt\n",
    "tb_melt = pd.melt(tb, id_vars=['country', 'year'])\n",
    "\n",
    "# Create the 'gender' column\n",
    "tb_melt['gender'] = tb_melt.variable.str[0]\n",
    "\n",
    "# Create the 'age_group' column\n",
    "tb_melt['age_group'] = tb_melt.variable.str[1:]\n",
    "\n",
    "# Print the head of tb_melt\n",
    "print(tb_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting a column with .split() and .get() (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')\n",
    "\n",
    "# Create the 'str_split' column\n",
    "ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')\n",
    "\n",
    "# Create the 'type' column\n",
    "ebola_melt['type'] = ebola_melt.str_split.str.get(0)\n",
    "\n",
    "# Create the 'country' column\n",
    "ebola_melt['country'] = ebola_melt.str_split.str.get(1)\n",
    "\n",
    "# Print the head of ebola_melt\n",
    "print(ebola_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate uber1, uber2, and uber3: row_concat\n",
    "row_concat = pd.concat([uber1,uber2,uber3])\n",
    "\n",
    "# Print the shape of row_concat\n",
    "print(row_concat.shape)\n",
    "\n",
    "# Print the head of row_concat\n",
    "print(row_concat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining columns of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate ebola_melt and status_country column-wise: ebola_tidy\n",
    "ebola_tidy = pd.concat([ebola_melt, status_country], axis=1)\n",
    "\n",
    "# Print the shape of ebola_tidy\n",
    "print(ebola_tidy.shape)\n",
    "\n",
    "# Print the head of ebola_tidy\n",
    "print(ebola_tidy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the DataFrames: o2o\n",
    "o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Print o2o\n",
    "print(o2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-1 data merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the DataFrames: m2o (same as before ONLY data is different)\n",
    "m2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Print m2o\n",
    "print(m2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-many data merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge site and visited: m2m\n",
    "m2m = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Merge m2m and survey: m2m\n",
    "m2m = pd.merge(left=m2m, right=survey, left_on='ident', right_on='taken')\n",
    "\n",
    "# Print the first 20 lines of m2m\n",
    "print(m2m.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
