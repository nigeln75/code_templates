{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas depends upon and interoperates with NumPy, the Python library for fast numeric array computations\n",
    "# For example, you can use the DataFrame attribute values to represent a DataFrame df as a NumPy array\n",
    "# You can also pass pandas data structures to NumPy methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Slicing on rows, columns and strides using iloc for positional slicing or loc based on labels\n",
    "# This slices rows then columns then stride, e.g. iloc(:5,:) gives the first 5 rows of the dataframe and iloc(:5)\n",
    "df.iloc(:5,:)\n",
    "df.iloc(:-5,:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specifying head method\n",
    "df.head(50)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tail\n",
    "df.tail(10)\n",
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Info - number of rows and columns and type of columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Broadcasting to assign a value to a slice\n",
    "# Here :: indicates the slice is every 3rd row from 0 in the last column\n",
    "df.iloc[::3,-1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The columns of a dataframe are a specialised data structure called a 'series'\n",
    "# A series is a 1 dimensional labelled numpy array\n",
    "type(df['First column'])\n",
    "\n",
    "# The values attributes create a numpy array from the series\n",
    "df['First column'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataframes in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1) read in data\n",
    "# 2) create a dictionary ... where the keys of the dictionary become the column names in the resulting dataframe\n",
    "# 3) from lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful keyword options:\n",
    "# - names: assigning column labels\n",
    "# - index_col: assigning index \n",
    "# - parse_dates: parsing datetimes \n",
    "# - na_values: parsing NaNs\n",
    "\n",
    "# Read in the file: df1\n",
    "df1 = pd.read_csv('world_population.csv')\n",
    "\n",
    "# Create a list of the new column labels: new_labels\n",
    "new_labels = ['year','population']\n",
    "\n",
    "# Read in the file, specifying the header and names parameters: df2\n",
    "df2 = pd.read_csv('world_population.csv', header=0, names=new_labels)\n",
    "\n",
    "# Print both the DataFrames\n",
    "print(df1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not all data files are clean and tidy\n",
    "# Pandas provides methods for reading those not-so-perfect data files that you encounter far too often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the raw file as-is: df1\n",
    "df1 = pd.read_csv(file_messy)\n",
    "\n",
    "# Print the output of df1.head()\n",
    "print(df1.head())\n",
    "\n",
    "# Read in the file with the correct parameters: df2\n",
    "df2 = pd.read_csv(file_messy, delimiter=' ', header=3, comment='#')\n",
    "\n",
    "# Print the output of df2.head()\n",
    "print(df2.head())\n",
    "\n",
    "# Save the cleaned up DataFrame to a CSV file without the index\n",
    "df2.to_csv(file_clean, index=False)\n",
    "\n",
    "# Save the cleaned up DataFrame to an excel file without the index\n",
    "df2.to_excel('file_clean.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) from a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     city  signups  visitors weekday\n",
      "0  Austin        7       139     Sun\n",
      "1  Dallas       12       237     Sun\n",
      "2  Austin        3       326     Mon\n",
      "3  Dallas        5       456     Mon\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'weekday': ['Sun', 'Sun', 'Mon', 'Mon'],\n",
    "        'city': ['Austin', 'Dallas', 'Austin', 'Dallas'],\n",
    "        'visitors': [139, 237, 326, 456],\n",
    "        'signups': [7, 12, 3, 5]}\n",
    "users = pd.DataFrame(data)\n",
    "print(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) from lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('city', ['Austin', 'Dallas', 'Austin', 'Dallas']), ('signups', [7, 12, 3, 5]), ('visitors', [139, 237, 326, 456]), ('weekday', ['Sun', 'Sun', 'Mon', 'Mon'])]\n",
      "     city  signups  visitors weekday\n",
      "0  Austin        7       139     Sun\n",
      "1  Dallas       12       237     Sun\n",
      "2  Austin        3       326     Mon\n",
      "3  Dallas        5       456     Mon\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cities = ['Austin', 'Dallas', 'Austin', 'Dallas']\n",
    "signups = [7, 12, 3, 5]\n",
    "visitors = [139, 237, 326, 456]\n",
    "weekdays = ['Sun', 'Sun', 'Mon', 'Mon']\n",
    "list_labels = ['city', 'signups', 'visitors', 'weekday']\n",
    "list_cols = [cities, signups, visitors, weekdays]\n",
    "# The list and zip functions create a list of tuples\n",
    "zipped = list(zip(list_labels, list_cols))\n",
    "print(zipped)\n",
    "# The dict function then converts the lists to a dictionary\n",
    "data = dict(zipped)  \n",
    "users = pd.DataFrame(data)\n",
    "print(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zip the 2 lists together into one list of (key,value) tuples: zipped\n",
    "zipped = list(zip(list_keys,list_values))\n",
    "\n",
    "# Inspect the list using print()\n",
    "print(zipped)\n",
    "\n",
    "# Build a dictionary with the zipped list: data\n",
    "data = dict(zipped)\n",
    "\n",
    "# Build and inspect a DataFrame from the dictionary: df\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Broadcasting\n",
    "# Add a new column\n",
    "users['fees'] = 0 # Broadcasts to entire column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Broadcasting with a dict\n",
    "import pandas as pd\n",
    "heights = [ 59.0, 65.2, 62.9, 65.4, 63.7, 65.7, 64.1 ]\n",
    "data = {'height': heights, 'sex': 'M'}\n",
    "# Here the value 'M' is broadcast to the entire column\n",
    "results = pd.DataFrame(data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can implicitly use 'broadcasting', a feature of NumPy, when creating pandas DataFrames\n",
    "# Make a string with the value 'PA': state\n",
    "state = 'PA'\n",
    "\n",
    "# Construct a dictionary: data\n",
    "data = {'state': state, 'city':cities}\n",
    "\n",
    "# Construct a DataFrame from dictionary data: df\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the columns and indexs using the .columns and .index attributes of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indexs and columns\n",
    "results.columns = ['height (in)', 'sex']\n",
    "# We can assign lists of strings as the row index as long as they are of the right length!\n",
    "results.index = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can use the DataFrame attribute df.columns to view and assign new string labels to columns in a pandas DataFrame\n",
    "# Build a list of labels: list_labels\n",
    "list_labels = ['year','artist','song','chart weeks']\n",
    "\n",
    "# Assign the list of labels to the columns attribute: df.columns\n",
    "df.columns = list_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic plotting - see also data visualisation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot all columns (default)\n",
    "df.plot()\n",
    "plt.show()\n",
    "\n",
    "# Plot all columns as subplots\n",
    "df.plot(subplots=True)\n",
    "plt.show()\n",
    "\n",
    "# Plot just the Dew Point data\n",
    "column_list1 = [\"Dew Point (deg F)\"]\n",
    "df[column_list1].plot()\n",
    "plt.show()\n",
    "\n",
    "# Plot the Dew Point and Temperature data, but not the Pressure data\n",
    "column_list2 = ['Temperature (deg F)','Dew Point (deg F)']\n",
    "df[column_list2].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of y-axis column names: y_columns\n",
    "y_columns = ['AAPL', 'IBM']\n",
    "\n",
    "# Generate a line plot\n",
    "df.plot(x='Month', y=y_columns)\n",
    "\n",
    "# Add the title\n",
    "plt.title('Monthly stock prices')\n",
    "\n",
    "# Add the y-axis label\n",
    "plt.ylabel('Price ($US)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a scatter plot\n",
    "df.plot(kind='scatter', x='hp', y='mpg', s=sizes)\n",
    "\n",
    "# Add the title\n",
    "plt.title('Fuel efficiency vs Horse-power')\n",
    "\n",
    "# Add the x-axis label\n",
    "plt.xlabel('Horse-power')\n",
    "\n",
    "# Add the y-axis label\n",
    "plt.ylabel('Fuel efficiency (mpg)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a list of the column names to be plotted: cols\n",
    "cols = ['weight', 'mpg']\n",
    "\n",
    "# Generate the box plots\n",
    "df[cols].plot(kind='box', subplots=True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms of PDF and CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This formats the plots such that they appear on separate rows\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1)\n",
    "\n",
    "# Plot the PDF\n",
    "df['fraction'].plot(ax=axes[0], kind='hist', normed=True, bins=30, range=(0,.3))\n",
    "plt.show()\n",
    "\n",
    "# Plot the CDF\n",
    "df['fraction'].plot(ax=axes[1], kind='hist', normed=True, cumulative=True, bins=30, range=(0,.3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lists of summary stats: counts (number of non-null stats),mean (ignores null entries), stds, quartiles\n",
    "df.describe()\n",
    "df.count()\n",
    "df.mean()\n",
    "df.std()\n",
    "df.median()\n",
    "# Quantile is another name for a percentile as a fraction. The default is the median\n",
    "df.quantile()\n",
    "# Specifying q as a list of 0.25 and 0.75 returns both 25th and 75th percentiles or the interquartile range\n",
    "q = [0.25,0.75]\n",
    "df.quantile(q)\n",
    "# Min and max\n",
    "df.max()\n",
    "df.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the minimum value of the Engineering column\n",
    "print(df['Engineering'].min())\n",
    "\n",
    "# Print the maximum value of the Engineering column\n",
    "print(df['Engineering'].max())\n",
    "\n",
    "# Construct the mean percentage per year: mean\n",
    "mean = df.mean(axis='columns')\n",
    "\n",
    "# Plot the average percentage per year\n",
    "mean.plot()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In many data sets, there can be large differences in the mean and median value due to the presence of outliers\n",
    "# Print summary statistics of the fare column with .describe()\n",
    "print(df['fare'].describe())\n",
    "\n",
    "# Generate a box plot of the fare column\n",
    "df['fare'].plot(kind='box')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the number of countries reported in 2015\n",
    "print(df['2015'].count())\n",
    "\n",
    "# Print the 5th and 95th percentiles\n",
    "print(df.quantile([0.05,0.95]))\n",
    "\n",
    "# Generate a box plot\n",
    "years = ['1800','1850','1900','1950','2000']\n",
    "df[years].plot(kind='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the mean of the January and March data\n",
    "print(january.mean(), march.mean())\n",
    "\n",
    "# Print the standard deviation of the January and March data\n",
    "print(january.std(), march.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniques and factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['A'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filtering by species\n",
    "indices = iris['species'] == 'setosa'\n",
    "setosa = iris.loc[indices,:] # extract new DataFrame\n",
    "indices = iris['species'] == 'versicolor'\n",
    "versicolor = iris.loc[indices,:] # extract new DataFrame\n",
    "indices = iris['species'] == 'virginica'\n",
    "virginica = iris.loc[indices,:] # extract new DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the differences in statistics between different groups within a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the global mean and global standard deviation: global_mean, global_std\n",
    "global_mean = df.mean()\n",
    "global_std = df.std()\n",
    "\n",
    "# Filter the US population from the origin column: us\n",
    "us = df[df['origin'] == 'US']\n",
    "\n",
    "# Compute the US mean and US standard deviation: us_mean, us_std\n",
    "us_mean = us.mean()\n",
    "us_std = us.std()\n",
    "\n",
    "# Print the differences\n",
    "print(us_mean - global_mean)\n",
    "print(us_std - global_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Population filtering can be used alongside plotting to quickly determine differences in distributions between the sub-populations\n",
    "# Display the box plots on 3 separate rows and 1 column\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "\n",
    "# Generate a box plot of the fare prices for the First passenger class\n",
    "titanic.loc[titanic['pclass'] == 1].plot(ax=axes[0], y='fare', kind='box')\n",
    "\n",
    "# Generate a box plot of the fare prices for the Second passenger class\n",
    "titanic.loc[titanic['pclass'] == 2].plot(ax=axes[1], y='fare', kind='box')\n",
    "\n",
    "# Generate a box plot of the fare prices for the Third passenger class\n",
    "titanic.loc[titanic['pclass'] == 3].plot(ax=axes[2], y='fare', kind='box')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing pandas time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pandas to read datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read_csv() function can read strings into datetime objects. You eed to specify ‘parse_dates=True’ \n",
    "#ISO 8601 format which is yyyy-mm-dd hh:mm:ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv('sales-feb-2015.csv', parse_dates=True, index_col = 'Date')\n",
    "sales.head()\n",
    "sales.info()\n",
    "# Selecting a single datetime\n",
    "sales.loc['2015-02-19 11:00:00', 'Company']\n",
    "# Selecting a whole day\n",
    "sales.loc['2015-2-5']\n",
    "# Partial datetime string selection\n",
    "# Alternative ways of selecting days\n",
    "sales.loc[‘February 5, 2015’]\n",
    "sales.loc[‘2015-Feb-5’] \n",
    "# Selecting a whole month\n",
    "sales.loc[‘2015-2’]\n",
    "# Selecting a whole year\n",
    "sales.loc[‘2015’]\n",
    "# Slicing using datetimes\n",
    "sales.loc['2015-2-16':'2015-2-20']\n",
    "# Converting strings to datetimes\n",
    "evening_2_11 = pd.to_datetime(['2015-2-11 20:00', '2015-2-11 21:00', '2015-2-11 22:00', '2015-2-11 23:00'])\n",
    "# Reindexing with forward fill\n",
    "sales.reindex(evening_2_11, method='ffill')\n",
    "# Reindexing with back fill\n",
    "sales.reindex(evening_2_11, method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare a format string: time_format\n",
    "time_format = '%Y-%m-%d %H:%M'\n",
    "\n",
    "# Convert date_list into a datetime object: my_datetimes\n",
    "my_datetimes = pd.to_datetime(date_list, format=time_format)  \n",
    "\n",
    "# Construct a pandas Series using temperature_list and my_datetimes: time_series\n",
    "time_series = pd.Series(temperature_list, index=my_datetimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing ranges of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the hour from 9pm to 10pm on '2010-10-11': ts1\n",
    "ts1 = ts0.loc['2010-10-11 21:00:00']\n",
    "\n",
    "# Extract '2010-07-04' from ts0: ts2\n",
    "ts2 = ts0.loc['2010-07-04']\n",
    "\n",
    "# Extract data from '2010-12-15' to '2010-12-31': ts3\n",
    "ts3 = ts0.loc['2010-12-15':'2010-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "climate2010['2010-05-31 22:00:00'] # datetime\n",
    "climate2010['2010-06-01'] # Entire day\n",
    "climate2010['2010-04'] # Entire month\n",
    "climate2010[‘2010-09':'2010-10'] # 2 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reindexing time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reindexing is useful in preparation for adding or otherwise combining two time series data sets. \n",
    "# To reindex the data, we provide a new index and ask pandas to try and match the old data to the new index. \n",
    "# If data is unavailble for one of the new index dates or times, you must tell pandas how to fill it in. \n",
    "# Otherwise, pandas will fill with NaN by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reindex without fill method: ts3\n",
    "ts3 = ts2.reindex(ts1.index)\n",
    "\n",
    "# Reindex with fill method, using forward fill: ts4\n",
    "ts4 = ts2.reindex(ts1.index, method='ffill')\n",
    "\n",
    "# Combine ts1 + ts2: sum12\n",
    "sum12 = ts1 + ts2\n",
    "\n",
    "# Combine ts1 + ts3: sum13\n",
    "sum13 = ts1 + ts3\n",
    "\n",
    "# Combine ts1 + ts4: sum14\n",
    "sum14 = ts1 + ts4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Statistical methods over different time intervals mean(), sum(), count(), etc.\n",
    "# Down-sampling reduces datetime rows to slower frequency\n",
    "# Up-sampling increases datetime rows to faster frequency\n",
    "# Aggregating means\n",
    "daily_mean = sales.resample('D').mean()\n",
    "# Summing daily sales using method chaining\n",
    "sales.resample('D').sum()\n",
    "# Calculating maximum daily sales using method chaining\n",
    "sales.resample('D').sum().max()\n",
    "# Resampling on strings, e.g. counting variables by week\n",
    "sales.resample('W').count()\n",
    "# Other option: \n",
    "# ‘min’, ‘ T’: minute\n",
    "# ‘H’: hour\n",
    "# ‘D’: day\n",
    "# ‘B’: business day\n",
    "# ‘W’: week\n",
    "# ‘M’: month\n",
    "# ‘Q’: quarter\n",
    "# ‘A’: year\n",
    "\n",
    "# Calculating sum of units every fortnight!\n",
    "sales.loc[:,'Units'].resample('2W').sum()\n",
    "\n",
    "# Up-sampling and filling\n",
    "two_days = sales.loc['2015-2-4': '2015-2-5', 'Units']\n",
    "two_days.resample('4H').ffill()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When downsampling or upsampling, the syntax is similar, but the methods called are different. \n",
    "# Both use the concept of 'method chaining' - df.method1().method2().method3() - to direct the output from one method call to the input of the next, and so on, as a sequence of operations, one feeding into the next.\n",
    "\n",
    "# Downsample to 6 hour data and aggregate by mean: df1\n",
    "df1 = df['Temperature'].resample('6h').mean()\n",
    "\n",
    "# Downsample to daily data and count the number of data points: df2\n",
    "df2 = df['Temperature'].resample('D').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating and resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract temperature data for August: august\n",
    "august = df['Temperature']['2010-August']\n",
    "\n",
    "# Downsample to obtain only the daily highest temperatures in August: august_highs\n",
    "august_highs = august.resample('D').max()\n",
    "\n",
    "# Extract temperature data for February: february\n",
    "february = df['Temperature']['2010-Feb']\n",
    "\n",
    "# Downsample to obtain the daily lowest temperatures in February: february_lows\n",
    "february_lows = february.resample('D').min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rolling mean and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rolling means (or moving averages) are generally used to smooth out short-term fluctuations in time series data and highlight long-term trends\n",
    "# To use the .rolling() method, you must always use method chaining, first calling .rolling() and then chaining an aggregation method after it\n",
    "# For example, with a Series hourly_data, hourly_data.rolling(window=24).mean() would compute new values for each hourly point, based on a 24-hour window stretching out behind each point\n",
    "# The frequency of the output data is the same: it is still hourly. \n",
    "# Such an operation is useful for smoothing time series data.\n",
    "\n",
    "# Extract data from 2010-Aug-01 to 2010-Aug-15: unsmoothed\n",
    "unsmoothed = df['Temperature']['2010-Aug-01':'2010-Aug-15']\n",
    "\n",
    "# Apply a rolling mean with a 24 hour window: smoothed\n",
    "smoothed = unsmoothed.rolling(window=24).mean()\n",
    "\n",
    "# Create a new DataFrame with columns smoothed and unsmoothed: august\n",
    "august = pd.DataFrame({'smoothed':smoothed, 'unsmoothed':unsmoothed})\n",
    "\n",
    "# Plot both smoothed and unsmoothed data using august.plot().\n",
    "august.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling and rolling average chained together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resample and roll with it\n",
    "# Extract the August 2010 data: august\n",
    "august = df['Temperature']['2010-8']\n",
    "\n",
    "# Resample to daily data, aggregating by max: daily_highs\n",
    "daily_highs = august.resample('D').max()\n",
    "\n",
    "# Use a rolling 7-day window with method chaining to smooth the daily high temperatures in August\n",
    "daily_highs_smoothed = august.resample('D').max().rolling(window=7).mean()\n",
    "print(daily_highs_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using dt to convert dates\n",
    "sales['Date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Boolean reduction\n",
    "sales['Product'].str.contains('ware').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Strip extra whitespace from the column names: df.columns\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Extract data for which the destination airport is Dallas: dallas\n",
    "dallas = df['Destination Airport'].str.contains('DAL')\n",
    "\n",
    "# Compute the total number of Dallas departures each day: daily_departures\n",
    "daily_departures = dallas.resample('D').sum()\n",
    "\n",
    "# Generate the summary statistics for daily Dallas departures: stats\n",
    "stats = daily_departures.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values and interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear interpolation (other method available!)\n",
    "population.resample('A').first().interpolate('linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset the index of ts2 to ts1, and then use linear interpolation to fill in the NaNs: ts2_interp\n",
    "ts2_interp = ts2.reindex(ts1.index).interpolate(how='linear')\n",
    "\n",
    "# Compute the absolute difference of ts1 and ts2_interp: differences \n",
    "differences = np.abs(ts1 - ts2_interp)\n",
    "\n",
    "# Generate and print summary statistics of the differences\n",
    "print(differences.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time zones and conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Time zone handling with pandas typically assumes that you are handling the Index of the Series\n",
    "# Localise time zone\n",
    "central = sales['Date'].dt.tz_localize('US/Central')\n",
    "# Convert time zone\n",
    "central.dt.tz_convert('US/Eastern')\n",
    "# Method chaining to localise and convert time zones in one go!\n",
    "sales['Date'].dt.tz_localize('US/Central').dt.tz_convert('US/Eastern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Buid a Boolean mask to filter out all the 'LAX' departure flights: mask\n",
    "mask = df['Destination Airport'] == 'LAX'\n",
    "\n",
    "# Use the mask to subset the data: la\n",
    "la = df[mask]\n",
    "\n",
    "# Combine two columns of data to create a datetime series: times_tz_none \n",
    "times_tz_none = pd.to_datetime( la['Date (MM/DD/YYYY)'] + ' ' + la['Wheels-off Time'])\n",
    "\n",
    "# Localize the time to US/Central: times_tz_central\n",
    "times_tz_central = times_tz_none.dt.tz_localize('US/Central')\n",
    "\n",
    "# Convert the datetimes from US/Central to US/Pacific\n",
    "times_tz_pacific = times_tz_central.dt.tz_convert('US/Pacific')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp500.loc['2012', ['Close','Volume']].plot(subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting time series, datetime indexing\n",
    "\n",
    "# Plot the raw data before setting the datetime index\n",
    "df.plot()\n",
    "plt.show()\n",
    "\n",
    "# Convert the 'Date' column into a collection of datetime objects: df.Date\n",
    "df.Date = pd.to_datetime(df.Date)\n",
    "\n",
    "# Set the index to be the converted 'Date' column\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Re-plot the DataFrame to see that the axis is now datetime aware!\n",
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting date ranges, partial indexing\n",
    "# Now that you have set the DatetimeIndex in your DataFrame, you have a much more powerful and flexible set of tools to use when plotting your time series data\n",
    "# Of these, one of the most convenient is partial string indexing and slicing\n",
    "\n",
    "# Plot the summer data\n",
    "df.Temperature['2010-Jun':'2010-Aug'].plot()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# Plot the one week data\n",
    "df.Temperature['2010-06-10':'2010-06-17'].plot()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study on weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the data file: df\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Print the output of df.head()\n",
    "print(df.head())\n",
    "\n",
    "# Read in the data file with header=None: df_headers\n",
    "df_headers = pd.read_csv(file_name, header=None)\n",
    "\n",
    "# Print the output of df_headers.head()\n",
    "print(df_headers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split on the comma to create a list: column_labels_list\n",
    "column_labels_list = column_labels.split(',')\n",
    "\n",
    "# Assign the new column labels to the DataFrame: df.columns\n",
    "df.columns = column_labels_list\n",
    "\n",
    "# Remove the appropriate columns: df_dropped\n",
    "df_dropped = df.drop(list_to_drop, axis='columns')\n",
    "\n",
    "# Print the output of df_dropped.head()\n",
    "print(df_dropped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and tidying datetime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to use the full power of pandas time series, you must construct a DatetimeIndex\n",
    "\n",
    "# Convert the date column to string: df_dropped['date']\n",
    "df_dropped['date'] = df_dropped['date'].astype(str)\n",
    "\n",
    "# Pad leading zeros to the Time column: df_dropped['Time']\n",
    "df_dropped['Time'] = df_dropped['Time'].apply(lambda x:'{:0>4}'.format(x))\n",
    "\n",
    "# Concatenate the new date and Time columns: date_string\n",
    "date_string = df_dropped['date'] + df_dropped['Time']\n",
    "\n",
    "# Convert the date_string Series to datetime: date_times\n",
    "date_times = pd.to_datetime(date_string, format='%Y%m%d%H%M')\n",
    "\n",
    "# Set the index to be the new date_times container: df_clean\n",
    "df_clean = df_dropped.set_index(date_times)\n",
    "\n",
    "# Print the output of df_clean.head()\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\n",
    "print(df_clean.loc['2011-6-20 8:00:00':'2011-6-20 9:00:00', 'dry_bulb_faren'])\n",
    "\n",
    "# Convert the dry_bulb_faren column to numeric values: df_clean['dry_bulb_faren']\n",
    "df_clean['dry_bulb_faren'] = pd.to_numeric(df_clean['dry_bulb_faren'], errors='coerce')\n",
    "\n",
    "# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011\n",
    "print(df_clean.loc['2011-6-20 8:00:00':'2011-6-20 9:00:00', 'dry_bulb_faren'])\n",
    "\n",
    "# Convert the wind_speed and dew_point_faren columns to numeric values\n",
    "df_clean['wind_speed'] = pd.to_numeric(df_clean['wind_speed'], errors='coerce')\n",
    "df_clean['dew_point_faren'] = pd.to_numeric(df_clean['dew_point_faren'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply statistical exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the median of the dry_bulb_faren column\n",
    "print(df_clean['dry_bulb_faren'].median())\n",
    "\n",
    "# Print the median of the dry_bulb_faren column for the time range '2011-Apr':'2011-Jun'\n",
    "print(df_clean.loc['2011-Apr':'2011-Jun', 'dry_bulb_faren'].median())\n",
    "\n",
    "# Print the median of the dry_bulb_faren column for the month of January\n",
    "print(df_clean.loc['2011-Jan', 'dry_bulb_faren'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downsample df_clean by day and aggregate by mean: daily_mean_2011\n",
    "daily_mean_2011 = df_clean.resample('D').mean()\n",
    "\n",
    "# Extract the dry_bulb_faren column from daily_mean_2011 using .values: daily_temp_2011\n",
    "daily_temp_2011 = daily_mean_2011['dry_bulb_faren'].values\n",
    "\n",
    "# Downsample df_climate by day and aggregate by mean: daily_mean_2011\n",
    "daily_climate = df_climate.resample('D').mean()\n",
    "\n",
    "# Extract the Temperature column from daily_climate using .reset_index(): daily_temp_climate\n",
    "daily_temp_climate = daily_climate.reset_index()['Temperature']\n",
    "\n",
    "# Compute the difference between the two arrays and print the mean difference\n",
    "difference = daily_temp_2011 - daily_temp_climate\n",
    "print(difference.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select days that are sunny: sunny\n",
    "sunny = df_clean.loc[df_clean['sky_condition']=='CLR']\n",
    "\n",
    "# Select days that are overcast: overcast\n",
    "overcast = df_clean.loc[df_clean['sky_condition'].str.contains('OVC')]\n",
    "\n",
    "# Resample sunny and overcast, aggregating by maximum daily temperature\n",
    "sunny_daily_max = sunny.resample('D').max()\n",
    "overcast_daily_max = overcast.resample('D').max()\n",
    "\n",
    "# Print the difference between the mean of sunny_daily_max and overcast_daily_max\n",
    "print(sunny_daily_max.mean() - overcast_daily_max.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply visual exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the visibility and dry_bulb_faren columns and resample them: weekly_mean\n",
    "weekly_mean = df_clean[['visibility','dry_bulb_faren']].resample('W').mean()\n",
    "\n",
    "# Print the output of weekly_mean.corr()\n",
    "print(weekly_mean.corr())\n",
    "\n",
    "# Plot weekly_mean with subplots=True\n",
    "weekly_mean.plot(subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Boolean Series for sunny days: sunny\n",
    "sunny = df_clean['sky_condition'] == 'CLR'\n",
    "\n",
    "# Resample the Boolean Series by day and compute the sum: sunny_hours\n",
    "sunny_hours = sunny.resample('D').sum()\n",
    "\n",
    "# Resample the Boolean Series by day and compute the count: total_hours\n",
    "total_hours = sunny.resample('D').count()\n",
    "\n",
    "# Divide sunny_hours by total_hours: sunny_fraction\n",
    "sunny_fraction = sunny_hours / total_hours\n",
    "\n",
    "# Make a box plot of sunny_fraction\n",
    "sunny_fraction.plot(kind='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resample dew_point_faren and dry_bulb_faren by Month, aggregating the maximum values: monthly_max\n",
    "monthly_max = df_clean[['dew_point_faren','dry_bulb_faren']].resample('M').max()\n",
    "\n",
    "# Generate a histogram with bins=8, alpha=0.5, subplots=True\n",
    "monthly_max.plot(kind='hist', bins=8, alpha=0.5, subplots=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the maximum temperature in August 2010 from df_climate: august_max\n",
    "august_max = df_climate.loc['2010-Aug','Temperature'].max()\n",
    "print(august_max)\n",
    "\n",
    "# Resample the August 2011 temperatures in df_clean by day and aggregate the maximum value: august_2011\n",
    "august_2011 = df_clean.loc['2011-Aug','dry_bulb_faren'].resample('D').max()\n",
    "\n",
    "# Filter out days in august_2011 where the value exceeded august_max: august_2011_high\n",
    "august_2011_high = august_2011.loc[august_2011>august_max]\n",
    "\n",
    "# Construct a CDF of august_2011_high\n",
    "august_2011_high.plot(kind='hist', bins=25, normed=True, cumulative=True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
